{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPchlntxSs4QY+KIypSw7bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/DeepLearningZeroToAll/blob/main/Lab06_SoftmaxClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab-06 Softmax Classifier\n",
        "# Logistic Regression 참조하여 구성\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x_data = [[1, 2, 1, 1],\n",
        "          [2, 1, 3, 2],\n",
        "          [3, 1, 3, 4],\n",
        "          [4, 1, 5, 5],\n",
        "          [1, 7, 5, 5],\n",
        "          [1, 2, 5, 6],\n",
        "          [1, 6, 6, 6],\n",
        "          [1, 7, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "x_data = np.asarray(x_data, dtype = np.float32)\n",
        "y_data = np.asarray(y_data, dtype = np.float32)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
        "W = tf.Variable(tf.random.normal([4,3], name='weight'))\n",
        "b = tf.Variable(tf.random.normal([3]),name = 'bias')\n",
        "variable = [W, b]\n",
        "\n",
        "# 원소의 자료구조 반환\n",
        "dataset.element_spec\n",
        "\n",
        "def softmax_fn (features):\n",
        "    hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn (features, labels):\n",
        "    hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n",
        "    cost = tf.reduce_mean(-tf.reduce_sum(y_data * tf.math.log(hypothesis), axis =1))\n",
        "    return cost\n",
        "\n",
        "def grad (hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(features, labels)\n",
        "    return tape.gradient(loss_value, [W, b])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
        "\n",
        "EPOCHS = 3000\n",
        "\n",
        "for step in range(EPOCHS + 1):\n",
        "    for features, labels in iter(dataset):\n",
        "        hypothesis = softmax_fn(features)\n",
        "        grads = grad(hypothesis, features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars = zip(grads, [W, b]))\n",
        "        if step % 300 == 0:\n",
        "                print(\"iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\n",
        "\n",
        "a = x_data\n",
        "a = softmax_fn (a)\n",
        "print(hypothesis) # softmax 함수를 통과시킨 x_data\n",
        "\n",
        "# argmax 가장 큰 값의 index를 찾아줌\n",
        "\n",
        "print(tf.argmax(a,1)) # 가설을 통한 예측값\n",
        "print(tf.argmax(y_data,1)) # 실제 값"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP67v1UmDRMD",
        "outputId": "358d3090-161b-4401-b5a6-72fae974c4e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0, Loss: 6.4214\n",
            "iter: 300, Loss: 0.4962\n",
            "iter: 600, Loss: 0.4649\n",
            "iter: 900, Loss: 0.4482\n",
            "iter: 1200, Loss: 0.4339\n",
            "iter: 1500, Loss: 0.4211\n",
            "iter: 1800, Loss: 0.4093\n",
            "iter: 2100, Loss: 0.3984\n",
            "iter: 2400, Loss: 0.3883\n",
            "iter: 2700, Loss: 0.3788\n",
            "iter: 3000, Loss: 0.3698\n",
            "tf.Tensor(\n",
            "[[3.6314048e-03 2.8929424e-02 9.6743912e-01]\n",
            " [8.2057863e-03 1.4974450e-01 8.4204972e-01]\n",
            " [6.6882739e-04 3.6115184e-01 6.3817924e-01]\n",
            " [6.1523635e-04 6.8219411e-01 3.1719074e-01]\n",
            " [5.2363032e-01 4.4592294e-01 3.0446663e-02]\n",
            " [2.6981068e-01 7.2880918e-01 1.3801689e-03]\n",
            " [6.1499602e-01 3.8297424e-01 2.0296504e-03]\n",
            " [7.3189980e-01 2.6783043e-01 2.6977173e-04]], shape=(8, 3), dtype=float32)\n",
            "tf.Tensor([2 2 2 1 0 1 0 0], shape=(8,), dtype=int64)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 세팅\n",
        "- 3개의 클래스 분류할때 0,1,2 를 각각 [1,0,0], [0,1,0], [0,0,1]로 one-hot-encoding\n",
        "- y의 개수 = 클래스 개수 = label개수\n",
        "<br>dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))<br>\n",
        "- tf.data.Dataset 파이프라인을 이용하여 값을 입력\n",
        "- from_tensor_slices 클래스 매서드를 사용하면 리스트, 넘파이, 텐서플로우 자료형에서 데이터셋을 만들 수 있음\n",
        "\n",
        "\n",
        "### <br>def softmax_fn (features):\n",
        "- 소프트맥스 함수 설정\n",
        "- hypothesis = tf.nn.softmax(tf.matmul(features,W)+b)\n",
        "\n",
        "### <br>def loss_fn(features, labels):\n",
        "- 비용 반환 함수.\n",
        "\n",
        "### <br> def grad(hypothesis, features, labels):\n",
        "- 기울기를 구하며 변수의 정보들을 tape에 기록.\n",
        "\n",
        "### <br> 학습\n",
        "optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n",
        "-  epoch를 돌면서 optimizer를 통해 지속적으로 최적값 update. -> 최적값을 찾아감."
      ],
      "metadata": {
        "id": "EZmfp-lAElWJ"
      }
    }
  ]
}
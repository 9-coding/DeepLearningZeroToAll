{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJugLijw0MnNOEXqdKaDAJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/DeepLearningZeroToAll/blob/main/Lab02_04_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOY2nb2FjB7C",
        "outputId": "d36c594f-8bde-4ceb-9724-0b6e02aea3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0|    2.4520|    0.3760| 45.660004\n",
            "   10|    1.1036|    0.0034|  0.206336\n",
            "   20|    1.0128|   -0.0209|  0.001026\n",
            "   30|    1.0065|   -0.0218|  0.000093\n",
            "   40|    1.0059|   -0.0212|  0.000083\n",
            "   50|    1.0057|   -0.0205|  0.000077\n",
            "   60|    1.0055|   -0.0198|  0.000072\n",
            "   70|    1.0053|   -0.0192|  0.000067\n",
            "   80|    1.0051|   -0.0185|  0.000063\n",
            "   90|    1.0050|   -0.0179|  0.000059\n",
            "  100|    1.0048|   -0.0173|  0.000055\n",
            "\n",
            "predict\n",
            "tf.Tensor(5.00667, shape=(), dtype=float32)\n",
            "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Lab-02 Simple Linear Regression\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1,2,3,4,5]\n",
        "y_data = [1,2,3,4,5]\n",
        "\n",
        "# 초기값 설정\n",
        "W = tf.Variable(2.9)\n",
        "b = tf.Variable(0.5)\n",
        "\n",
        "# learning_rate initialize\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "# Gradient descent\n",
        "for i in range(101):\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = W * x_data + b\n",
        "    # MSE\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "  #\n",
        "  if i % 10 == 0:\n",
        "    print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
        "\n",
        "# Predict\n",
        "print(\"\\npredict\")\n",
        "print(W * 5 + b)\n",
        "print(W * 2.5 + b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "tf.reduce_mean() : 차원이 줄어들면서 평균을 구한다는 의미.\n",
        "\n",
        "tf.square() : 괄호 안에서 제곱 구함.\n",
        "\n",
        "<br>\n",
        "\n",
        "# Gradient descent\n",
        "with tf.GradientTape() as tape:\n",
        "\n",
        "> with 내부의 변수들의 정보를 tape에 기록.\n",
        "\n",
        "tape.gradient()\n",
        "\n",
        "- 변수들의 미분값(기울기값) 계산.\n",
        "- W와 b에 대한 미분값을 W_grad와 b_grad에 각각 할당.\n",
        "\n",
        "A.assign_sub(B)\n",
        "\n",
        "> A = A - B<br>\n",
        "A -= B<br>\n",
        "와 같은 기능.<br>\n",
        "이것들을 텐서플로우에서 사용할 수 없기 때문에 이러한 메소드를 사용함.\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "W.assign_sub(learning_rate * W_grad)<br>\n",
        "b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        ": 기울기를 얼마나 반영시킬지 결정함. learning_rate가 크다면 많이 반영하고 적다면 적게 반영.\n",
        "\n"
      ],
      "metadata": {
        "id": "2yy3O_qkkxqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab-03 Linear Regression and How to minimize cost\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Cost function in pure Python\n",
        "X = np.array([1,2,3])\n",
        "Y = np.array([1,2,3])\n",
        "\n",
        "def cost_func(W,X,Y):\n",
        "  c = 0\n",
        "  for i in range(len(X)):\n",
        "    c += (W * X[i] - Y[i]) ** 2\n",
        "  return c / len(X)\n",
        "\n",
        "for feed_W in np.linspace(-3,5,num=15):\n",
        "  curr_cost = cost_func(feed_W, X, Y)\n",
        "  print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))\n",
        "\n",
        "# Cost function in tensorflow\n",
        "X = np.array([1,2,3])\n",
        "Y = np.array([1,2,3])\n",
        "\n",
        "def cost_func(W,X,Y):\n",
        "  hypothesis = X * W\n",
        "  return tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "\n",
        "W_values = np.linspace(-3,5,num=15)\n",
        "cost_values = []\n",
        "\n",
        "for feed_W in W_values:\n",
        "  curr_cost = cost_func(feed_W, X, Y)\n",
        "  cost_values.append(curr_cost)\n",
        "  print(\"{:6.3f} | {:10.5f}\".format(feed_W, curr_cost))\n",
        "\n",
        "# Gradient\n",
        "alpha = 0.01\n",
        "gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
        "descent = W - tf.multiply(alpha, gradient)\n",
        "W.assign(descent)\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "tf.random.set_seed(0) # for reproducibility\n",
        "\n",
        "x_data = [1., 2., 3., 4.]\n",
        "y_data = [1., 3., 5., 7.]\n",
        "\n",
        "W = tf.Variable([5.0])\n",
        "# 정규분포를 따르는 변수를 생성\n",
        "\n",
        "for step in range(300):\n",
        "    hypothesis = W * x_data\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "    alpha = 0.01  # learning rate\n",
        "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, x_data) - y_data, x_data))\n",
        "    descent = W - tf.multiply(alpha, gradient)\n",
        "    W.assign(descent)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(\"{:5} | {:10.4f} | {:10.6f}\".format(step, cost.numpy(), W.numpy()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOhTUvg_pY23",
        "outputId": "3e9b9931-753d-4295-a58b-602657b92b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.000 |   74.66667\n",
            "-2.429 |   54.85714\n",
            "-1.857 |   38.09524\n",
            "-1.286 |   24.38095\n",
            "-0.714 |   13.71429\n",
            "-0.143 |    6.09524\n",
            " 0.429 |    1.52381\n",
            " 1.000 |    0.00000\n",
            " 1.571 |    1.52381\n",
            " 2.143 |    6.09524\n",
            " 2.714 |   13.71429\n",
            " 3.286 |   24.38095\n",
            " 3.857 |   38.09524\n",
            " 4.429 |   54.85714\n",
            " 5.000 |   74.66667\n",
            "-3.000 |   74.66667\n",
            "-2.429 |   54.85714\n",
            "-1.857 |   38.09524\n",
            "-1.286 |   24.38095\n",
            "-0.714 |   13.71429\n",
            "-0.143 |    6.09524\n",
            " 0.429 |    1.52381\n",
            " 1.000 |    0.00000\n",
            " 1.571 |    1.52381\n",
            " 2.143 |    6.09524\n",
            " 2.714 |   13.71429\n",
            " 3.286 |   24.38095\n",
            " 3.857 |   38.09524\n",
            " 4.429 |   54.85714\n",
            " 5.000 |   74.66667\n",
            "\n",
            "    0 |    83.5000 |   4.750000\n",
            "   10 |    17.6915 |   3.080629\n",
            "   20 |     3.8521 |   2.315085\n",
            "   30 |     0.9417 |   1.964020\n",
            "   40 |     0.3297 |   1.803027\n",
            "   50 |     0.2009 |   1.729199\n",
            "   60 |     0.1739 |   1.695343\n",
            "   70 |     0.1682 |   1.679817\n",
            "   80 |     0.1670 |   1.672697\n",
            "   90 |     0.1667 |   1.669432\n",
            "  100 |     0.1667 |   1.667935\n",
            "  110 |     0.1667 |   1.667248\n",
            "  120 |     0.1667 |   1.666933\n",
            "  130 |     0.1667 |   1.666789\n",
            "  140 |     0.1667 |   1.666723\n",
            "  150 |     0.1667 |   1.666692\n",
            "  160 |     0.1667 |   1.666678\n",
            "  170 |     0.1667 |   1.666672\n",
            "  180 |     0.1667 |   1.666669\n",
            "  190 |     0.1667 |   1.666668\n",
            "  200 |     0.1667 |   1.666667\n",
            "  210 |     0.1667 |   1.666667\n",
            "  220 |     0.1667 |   1.666667\n",
            "  230 |     0.1667 |   1.666667\n",
            "  240 |     0.1667 |   1.666667\n",
            "  250 |     0.1667 |   1.666667\n",
            "  260 |     0.1667 |   1.666667\n",
            "  270 |     0.1667 |   1.666667\n",
            "  280 |     0.1667 |   1.666667\n",
            "  290 |     0.1667 |   1.666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost가 가장 작게 되는 W를 구하는 것이 중요. <br>\n",
        "cost function은 이차함수의 모양을 띄므로 미분값이 0이 되는 지점을 찾아서 구함\n",
        "\n",
        "Gradient descent algorithm\n",
        "경사하강법\n",
        "경사를 따라 내려가면서 최저점을 찾도록 설계된 알고리즘\n",
        "문제 해결의 대부분은 최적화 문제임. 이득 최대화 또는 손실 최대화.\n",
        "\n",
        "동작\n",
        "1. initial value를 정하고\n",
        "2. W와 b값을 cost가 줄어들 수 있는 방향(기울기를 사용하여)으로 지속적으로 update함.\n",
        "3. minimum에 도달할 때까지 반복.\n",
        "\n",
        "cost_func()\n",
        "가중치 feed_W에 따라서 어떻게 cost function이 그려지는지 curr_cost에 저장.\n"
      ],
      "metadata": {
        "id": "V_yjLkGjqvKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab-04 Multi variable linear regression LAB\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
        "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
        "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
        "Y  = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# weights\n",
        "w1 = tf.Variable(tf.random.normal([1]))\n",
        "w2 = tf.Variable(tf.random.normal([1]))\n",
        "w3 = tf.Variable(tf.random.normal([1]))\n",
        "b  = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1001):\n",
        "  # tf.GradientTape() to record the gradient of the cost function\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "  # calculates the gradients of the cost\n",
        "  w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "\n",
        "  # update w1, w2, w3 and b\n",
        "  w1.assign_sub(learning_rate * w1_grad)\n",
        "  w2.assign_sub(learning_rate * w2_grad)\n",
        "  w3.assign_sub(learning_rate * w3_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 50 == 0:\n",
        "    print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "data = np.array([\n",
        "    # x1,   x2,   x3,   y\n",
        "    [73.,  80.,  75., 152.],\n",
        "    [93.,  88.,  93., 185.],\n",
        "    [89.,  91.,  90., 180.],\n",
        "    [96.,  98., 100., 196.],\n",
        "    [73.,  66.,  70., 142.]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3, 1]))\n",
        "b = tf.Variable(tf.random.normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs + 1):\n",
        "    # record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "    # calculates the gradients of the cost\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    # update w1, w2, w3 and b\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkReV7GMfkvW",
        "outputId": "835184c1-c39a-4c9d-db65-20374baee993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 |   11325.9121\n",
            "   50 |     135.3618\n",
            "  100 |      11.1817\n",
            "  150 |       9.7940\n",
            "  200 |       9.7687\n",
            "  250 |       9.7587\n",
            "  300 |       9.7489\n",
            "  350 |       9.7389\n",
            "  400 |       9.7292\n",
            "  450 |       9.7194\n",
            "  500 |       9.7096\n",
            "  550 |       9.6999\n",
            "  600 |       9.6903\n",
            "  650 |       9.6806\n",
            "  700 |       9.6709\n",
            "  750 |       9.6612\n",
            "  800 |       9.6517\n",
            "  850 |       9.6421\n",
            "  900 |       9.6325\n",
            "  950 |       9.6229\n",
            " 1000 |       9.6134\n",
            "    0 |    5455.5903\n",
            "  100 |      31.7443\n",
            "  200 |      30.9326\n",
            "  300 |      30.7894\n",
            "  400 |      30.6468\n",
            "  500 |      30.5055\n",
            "  600 |      30.3644\n",
            "  700 |      30.2242\n",
            "  800 |      30.0849\n",
            "  900 |      29.9463\n",
            " 1000 |      29.8081\n",
            " 1100 |      29.6710\n",
            " 1200 |      29.5348\n",
            " 1300 |      29.3989\n",
            " 1400 |      29.2641\n",
            " 1500 |      29.1299\n",
            " 1600 |      28.9961\n",
            " 1700 |      28.8634\n",
            " 1800 |      28.7313\n",
            " 1900 |      28.5997\n",
            " 2000 |      28.4689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "변수가 늘어남에 따라 가중치도 똑같이 늘어나야 함.\n",
        "\n",
        "tf.matmul()\n",
        ": 행렬곱."
      ],
      "metadata": {
        "id": "dLtMPghqjD74"
      }
    }
  ]
}
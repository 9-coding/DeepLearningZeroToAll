{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/r8jjduiu77JfALHM/gwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9-coding/DeepLearningZeroToAll/blob/main/Lab05_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfgbD5AIDl1L",
        "outputId": "191cf3af-cc24-4547-c05d-872a2788c96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 0.6874\n",
            "Iter: 300, Loss: 0.5054\n",
            "Iter: 600, Loss: 0.4535\n",
            "Iter: 900, Loss: 0.4228\n",
            "Iter: 1200, Loss: 0.3992\n",
            "Iter: 1500, Loss: 0.3790\n",
            "Iter: 1800, Loss: 0.3608\n",
            "Iter: 2100, Loss: 0.3442\n",
            "Iter: 2400, Loss: 0.3288\n",
            "Iter: 2700, Loss: 0.3146\n",
            "Iter: 3000, Loss: 0.3013\n",
            "Accuracy: 100%\n"
          ]
        }
      ],
      "source": [
        "# Lab-05-3 Logistic Regression\n",
        "# 영상 코드 말고 아래의 댓글을 참조함.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x_train = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 1],\n",
        "    [4, 3],\n",
        "    [5, 3],\n",
        "    [6, 2]], dtype=np.float32)\n",
        "y_train = np.array([\n",
        "    [0],\n",
        "    [0],\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]], dtype=np.float32)\n",
        "\n",
        "x_test = np.array([[5, 2]], dtype=np.float32)\n",
        "y_test = np.array([[1]], dtype=np.float32)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
        "W = tf.Variable(tf.zeros([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')\n",
        "\n",
        "# 원소의 자료구조 반환\n",
        "dataset.element_spec\n",
        "\n",
        "# 회귀식 설정\n",
        "def logistic_regression(features):\n",
        "    hypothesis = tf.sigmoid(tf.matmul(features, W) + b)\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn(features, labels):\n",
        "    hypothesis = logistic_regression(features)\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(features, labels)\n",
        "    return tape.gradient(loss_value, [W,b])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "EPOCHS = 3000\n",
        "\n",
        "for step in range(EPOCHS + 1):\n",
        "    for features, labels in iter(dataset):\n",
        "        hypothesis = logistic_regression(features)\n",
        "        grads = grad(hypothesis, features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n",
        "        if step % 300 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\n",
        "            # print(hypothesis)\n",
        "\n",
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
        "    return accuracy\n",
        "\n",
        "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\n",
        "print('Accuracy: {}%'.format(test_acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 세팅\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))<br>\n",
        "- tf.data.Dataset 파이프라인을 이용하여 값을 입력\n",
        "- from_tensor_slices 클래스 매서드를 사용하면 리스트, 넘파이, 텐서플로우 자료형에서 데이터셋을 만들 수 있음\n",
        "\n",
        "\n",
        "### <br>def logistic_regression(features):\n",
        "- 시그모이드 함수 설정\n",
        "\n",
        "### <br>def loss_fn(features, labels):\n",
        "- 비용 반환 함수.\n",
        "- labels : 실제값 | hypothesis : 예측값.<br>\n",
        "- cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "  * 실제 레이블이 0과 1일 때를 설정하여 실제 레이블에 가까우면 줄어들도록 손실을 구함.\n",
        "  * 두 항을 더하고 평균을 구해 전체 데이터의 손실 계산.\n",
        "\n",
        "### <br> def grad(hypothesis, features, labels):\n",
        "- 기울기를 구하며 변수의 정보들을 tape에 기록.\n",
        "\n",
        "### <br> 학습\n",
        "optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\n",
        "-  epoch를 돌면서 optimizer를 통해 지속적으로 최적값 update. -> 최적값을 찾아감.\n",
        "\n",
        "### <br>def accuracy_fn(hypothesis, labels):\n",
        "- 0.5를 threshold로 잡고 예측값을 판단한 다음, 실제값과 일치하는지 확인."
      ],
      "metadata": {
        "id": "Wfd1q5rwHzDG"
      }
    }
  ]
}